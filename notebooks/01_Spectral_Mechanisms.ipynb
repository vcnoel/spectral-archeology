{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Figure 4 Reproduction: Spectral Mechanisms of Recovery\n",
                "\n",
                "This notebook reproduces the spectral analysis presented in Figure 4 of the paper. It compares different intervention strategies (Structural Steering, Chain-of-Thought, Sparsity, Windowing) on a single natural linguistic sample to demonstrate the \"Two Pathways to Recovery\" hypothesis. It also compares against the **\"Active\"** version of the sentence as the ideal baseline.\n",
                "\n",
                "**Sample:** \"The book was written by the man.\"\n",
                "**Active Target:** \"The man wrote the book.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
                "SAMPLE_TEXT = \"The book was written by the man.\"\n",
                "ACTIVE_TEXT = \"The man wrote the book.\"\n",
                "STEERING_LAYER_IDX = 2\n",
                "ALPHA = 0.2\n",
                "TOP_K = 2\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Model\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME, \n",
                "    torch_dtype=torch.float16, \n",
                "    device_map=\"auto\",\n",
                "    attn_implementation=\"eager\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Spectral Metric Functions\n",
                "def get_spectral_metrics(adj_matrix):\n",
                "    \"\"\"Compute Fiedler, HFER, Smoothness, and Entropy for a single attention matrix.\"\"\"\n",
                "    N = adj_matrix.shape[0]\n",
                "    if N < 2: return 0.0, 0.0, 0.0, 0.0\n",
                "    \n",
                "    # Laplacian\n",
                "    W = 0.5 * (adj_matrix + adj_matrix.T)\n",
                "    D = np.diag(np.sum(W, axis=1))\n",
                "    L = D - W\n",
                "    \n",
                "    try:\n",
                "        eigvals, eigvecs = np.linalg.eigh(L)\n",
                "        idx = eigvals.argsort()\n",
                "        eigvals = eigvals[idx]\n",
                "        eigvecs = eigvecs[:, idx]\n",
                "        \n",
                "        # 1. Fiedler Value (Algebraic Connectivity)\n",
                "        fiedler = eigvals[1] if N > 1 else 0.0\n",
                "        \n",
                "        # 2. Smoothness (of linear ramp signal)\n",
                "        x = np.linspace(-1, 1, N)\n",
                "        smoothness = (x.T @ L @ x) / (x.T @ x + 1e-9)\n",
                "        \n",
                "        # 3. HFER (High Frequency Energy Ratio)\n",
                "        c = eigvecs.T @ x\n",
                "        energy = c**2\n",
                "        total_energy = np.sum(energy)\n",
                "        k_cut = N // 2\n",
                "        high_energy = np.sum(energy[k_cut:])\n",
                "        hfer = high_energy / total_energy if total_energy > 0 else 0.0\n",
                "        \n",
                "    except:\n",
                "        fiedler, hfer, smoothness = 0.0, 0.0, 0.0\n",
                "\n",
                "    # 4. Entropy (Average Row Entropy)\n",
                "    row_entropies = []\n",
                "    for row in adj_matrix:\n",
                "        r = row + 1e-12\n",
                "        r = r / r.sum()\n",
                "        e = -np.sum(r * np.log(r))\n",
                "        row_entropies.append(e)\n",
                "    entropy = np.mean(row_entropies)\n",
                "\n",
                "    return fiedler, hfer, smoothness, entropy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calibration for Steering Vector\n",
                "def get_mean_hidden(texts, layer_idx):\n",
                "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs, output_hidden_states=True)\n",
                "    h_states = outputs.hidden_states[layer_idx + 1]\n",
                "    mask = inputs.attention_mask.unsqueeze(-1)\n",
                "    sum_h = (h_states * mask).sum(dim=1)\n",
                "    count_h = mask.sum(dim=1)\n",
                "    return (sum_h / count_h).mean(dim=0)\n",
                "\n",
                "# Tech Vocab for Robust Steering\n",
                "tech_pairs = [\n",
                "    (\"the system processed the data\", \"The data was processed by the system\"),\n",
                "    (\"the layer encoded the input\", \"The input was encoded by the layer\"),\n",
                "    (\"the network mapped the query\", \"The query was mapped by the network\"),\n",
                "]\n",
                "\n",
                "mu_act = get_mean_hidden([p[0] for p in tech_pairs], STEERING_LAYER_IDX)\n",
                "mu_pas = get_mean_hidden([p[1] for p in tech_pairs], STEERING_LAYER_IDX)\n",
                "steering_vector = mu_act - mu_pas\n",
                "print(\"Steering vector calibrated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(mode=\"baseline\"):\n",
                "    target_text = SAMPLE_TEXT\n",
                "    start_idx, end_idx = 0, None\n",
                "    \n",
                "    if mode == \"cot\":\n",
                "        prefix = \"Question: Rewrite the following sentence in active voice.\\nSentence: \"\n",
                "        suffix = \"\\nAnalysis: Let's identify the subject and object.\"\n",
                "        text_input = prefix + target_text + suffix\n",
                "        \n",
                "        inputs = tokenizer(text_input, return_tensors=\"pt\").to(model.device)\n",
                "        # Find target tokens slice\n",
                "        target_ids = tokenizer(target_text, add_special_tokens=False).input_ids\n",
                "        full_ids = inputs.input_ids[0].tolist()\n",
                "        \n",
                "        for i in range(len(full_ids) - len(target_ids) + 1):\n",
                "             if full_ids[i:i+len(target_ids)] == target_ids:\n",
                "                 start_idx = i\n",
                "                 break\n",
                "        end_idx = start_idx + len(target_ids)\n",
                "        \n",
                "    elif mode == \"active\":\n",
                "        text_input = ACTIVE_TEXT\n",
                "        inputs = tokenizer(text_input, return_tensors=\"pt\").to(model.device)\n",
                "        end_idx = inputs.input_ids.shape[1]\n",
                "        \n",
                "    else:\n",
                "        text_input = target_text\n",
                "        inputs = tokenizer(text_input, return_tensors=\"pt\").to(model.device)\n",
                "        end_idx = inputs.input_ids.shape[1]\n",
                "\n",
                "    # Hooks\n",
                "    hooks = []\n",
                "    if mode == \"structural\":\n",
                "        def steer_hook(module, inp, out):\n",
                "            if isinstance(out, tuple):\n",
                "                h = out[0]\n",
                "                v = steering_vector.view(1, 1, -1).to(h.dtype)\n",
                "                return (h + ALPHA * v,) + out[1:]\n",
                "            return out + ALPHA * steering_vector.view(1, 1, -1).to(out.dtype)\n",
                "        h = model.model.layers[STEERING_LAYER_IDX].register_forward_hook(steer_hook)\n",
                "        hooks.append(h)\n",
                "\n",
                "    # Forward Pass\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs, output_attentions=True)\n",
                "    for h in hooks: h.remove()\n",
                "\n",
                "    # Analyze Layers\n",
                "    layer_metrics = []\n",
                "    for i, layer_attn in enumerate(outputs.attentions):\n",
                "        # Mean over heads\n",
                "        A_full = layer_attn[0].mean(dim=0).float().cpu().numpy()\n",
                "        \n",
                "        # Slice CoT\n",
                "        limit = A_full.shape[0]\n",
                "        s = start_idx if start_idx < limit else 0\n",
                "        e = end_idx if end_idx and end_idx <= limit else limit\n",
                "        A = A_full[s:e, s:e]\n",
                "        \n",
                "        # Re-normalize\n",
                "        A = A / (A.sum(axis=1, keepdims=True) + 1e-12)\n",
                "        \n",
                "        # Apply Masks\n",
                "        if mode == \"window\":\n",
                "            mask = np.eye(A.shape[0], k=0) + np.eye(A.shape[0], k=1) + np.eye(A.shape[0], k=-1)\n",
                "            A = A * np.tril(mask)\n",
                "            A = A / (A.sum(axis=1, keepdims=True) + 1e-12)\n",
                "        elif mode == \"sparsity\":\n",
                "            for r in range(A.shape[0]):\n",
                "                row = A[r]\n",
                "                if len(row) > TOP_K:\n",
                "                    idx = np.argpartition(row, -TOP_K)[-TOP_K:]\n",
                "                    new_row = np.zeros_like(row)\n",
                "                    new_row[idx] = row[idx]\n",
                "                    A[r] = new_row\n",
                "            A = A / (A.sum(axis=1, keepdims=True) + 1e-12)\n",
                "\n",
                "        layer_metrics.append(get_spectral_metrics(A))\n",
                "    \n",
                "    return layer_metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Experiments\n",
                "results = {}\n",
                "modes = [\"baseline\", \"structural\", \"cot\", \"sparsity\", \"window\", \"active\"]\n",
                "\n",
                "for mode in modes:\n",
                "    print(f\"Running {mode}...\")\n",
                "    results[mode] = run_experiment(mode)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plotting\n",
                "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
                "metrics = [\"Fiedler\", \"HFER\", \"Smoothness\", \"Entropy\"]\n",
                "\n",
                "styles = {\n",
                "    \"baseline\": {\"color\": \"blue\", \"label\": \"Baseline\", \"linewidth\": 2, \"marker\": \"o\"},\n",
                "    \"structural\": {\"color\": \"orange\", \"label\": \"Structural\", \"linewidth\": 2, \"linestyle\": \"--\"},\n",
                "    \"cot\": {\"color\": \"gold\", \"label\": \"CoT\", \"linewidth\": 2, \"marker\": \"s\"},\n",
                "    \"sparsity\": {\"color\": \"grey\", \"label\": \"Sparsity\", \"linestyle\": \":\"},\n",
                "    \"window\": {\"color\": \"red\", \"label\": \"Window\", \"linestyle\": \"-.\"},\n",
                "    \"active\": {\"color\": \"green\", \"label\": \"Active\", \"linewidth\": 2}\n",
                "}\n",
                "\n",
                "for i, metric in enumerate(metrics):\n",
                "    ax = axes[i]\n",
                "    for mode in modes:\n",
                "        data = [x[i] for x in results[mode]]\n",
                "        style = styles.get(mode, {})\n",
                "        ax.plot(data, **style)\n",
                "    ax.set_title(metric)\n",
                "    ax.set_xlabel(\"Layer\")\n",
                "    if i == 0: ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}