{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Table 1 Reproduction: Surgical Repair Verification\n",
                "\n",
                "This notebook reproduces the **Table 1** results (Surgical Repair Performance) using the **canonical natural sample** discussed in the paper:\n",
                "> *\"The book was written by the man.\"*\n",
                "\n",
                "We compare the baseline performance (Passive) vs. the Steered performance (Repair) using a robust technical steering vector.\n",
                "\n",
                "**Note:** This notebook uses the `spectral_trust` library for standardized spectral metric calculations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from spectral_trust import GSPConfig, GraphConstructor, SpectralAnalyzer\n",
                "\n",
                "# Models to test\n",
                "MODELS = [\n",
                "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
                "    # Uncomment to run full suite\n",
                "    # \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
                "    # \"meta-llama/Llama-3.2-3B-Instruct\"\n",
                "]\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. TEST SAMPLES (Canonical & Natural) ---\n",
                "TEST_PAIRS = [\n",
                "    (\"The man wrote the book.\", \"The book was written by the man.\"),\n",
                "    (\"The teacher read the letter.\", \"The letter was read by the teacher.\"),\n",
                "    (\"The girl found the key.\", \"The key was found by the girl.\"),\n",
                "    (\"The doctor cured the patient.\", \"The patient was cured by the doctor.\"),\n",
                "    (\"The artist painted the picture.\", \"The picture was painted by the artist.\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2. SPECTRAL UTILS (Using spectral-trust) ---\n",
                "def get_fiedler_value(adj_matrix, graph_constructor, spectral_analyzer):\n",
                "    \"\"\"Compute Fiedler value using spectral-trust library.\"\"\"\n",
                "    if adj_matrix.shape[0] < 2: return 0.0\n",
                "    \n",
                "    # Convert to Tensor [1, seq, seq]\n",
                "    adj_tensor = torch.tensor(adj_matrix, dtype=torch.float32, device=device).unsqueeze(0)\n",
                "    \n",
                "    # Construct Laplacian (handles symmetrization)\n",
                "    L = graph_constructor.construct_laplacian(adj_tensor)\n",
                "    \n",
                "    # FIX: Move to CPU for SciPy based eigendecomposition\n",
                "    L = L.cpu()\n",
                "    \n",
                "    # Compute Eigenvalues\n",
                "    evals, _ = spectral_analyzer.compute_eigendecomposition(L)\n",
                "    \n",
                "    # Return lambda_2 (batch index 0)\n",
                "    return evals[0, 1].item() if evals.shape[1] > 1 else 0.0\n",
                "\n",
                "def calculate_perplexity(model, tokenizer, text):\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs, labels=inputs.input_ids)\n",
                "    return torch.exp(outputs.loss).item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3. EXPERIMENT LOOP ---\n",
                "RESULTS = []\n",
                "\n",
                "for model_name in MODELS:\n",
                "    print(f\"\\nProcessing {model_name}...\")\n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_name, \n",
                "            torch_dtype=torch.float16, \n",
                "            device_map=\"auto\",\n",
                "            attn_implementation=\"eager\"\n",
                "        )\n",
                "        \n",
                "        # Initialize Spectral Trust Tools\n",
                "        config = GSPConfig(model_name=model_name, device=device)\n",
                "        gc = GraphConstructor(config)\n",
                "        sa = SpectralAnalyzer(config)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Skipping {model_name}: {e}\")\n",
                "        continue\n",
                "\n",
                "    # CALIBRATE STEERING (Robust Technical Set)\n",
                "    LAYER_IDX = 2\n",
                "    technical_calibration = [\n",
                "        (\"The system processed the data.\", \"The data was processed by the system.\"),\n",
                "        (\"The server hosted the website.\", \"The website was hosted by the server.\"),\n",
                "        (\"The code compiled the program.\", \"The program was compiled by the code.\"),\n",
                "        (\"The layer encoded the input.\", \"The input was encoded by the layer.\"),\n",
                "        (\"The model predicted the token.\", \"The token was predicted by the model.\"),\n",
                "        (\"The node routed the packet.\", \"The packet was routed by the node.\"),\n",
                "        (\"The script executed the command.\", \"The command was executed by the script.\"),\n",
                "        (\"The database stored the record.\", \"The record was stored by the database.\")\n",
                "    ]\n",
                "    \n",
                "    def get_mean_hidden(texts):\n",
                "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
                "        with torch.no_grad(): outputs = model(**inputs, output_hidden_states=True)\n",
                "        h = outputs.hidden_states[LAYER_IDX + 1]\n",
                "        mask = inputs.attention_mask.unsqueeze(-1)\n",
                "        return ((h * mask).sum(dim=1) / mask.sum(dim=1)).mean(dim=0)\n",
                "    \n",
                "    v_act = get_mean_hidden([x[0] for x in technical_calibration])\n",
                "    v_pas = get_mean_hidden([x[1] for x in technical_calibration])\n",
                "    steering_vector = v_act - v_pas\n",
                "    print(\"Steering calibrated on Technical Set.\")\n",
                "    \n",
                "    # RUN TEST\n",
                "    ppl_bases, ppl_steers = [], []\n",
                "    f_bases, f_steers, f_actives = [], [], []\n",
                "    alpha = 0.2\n",
                "    \n",
                "    def steer_hook(module, inp, out):\n",
                "        if isinstance(out, tuple):\n",
                "            h = out[0]\n",
                "            v = steering_vector.view(1, 1, -1).to(h.dtype)\n",
                "            return (h + alpha * v,) + out[1:]\n",
                "        return out + alpha * steering_vector.view(1, 1, -1).to(out.dtype)\n",
                "    \n",
                "    for active, passive in TEST_PAIRS:\n",
                "        # Baseline\n",
                "        ppl_base = calculate_perplexity(model, tokenizer, passive)\n",
                "        inp = tokenizer(passive, return_tensors=\"pt\").to(model.device)\n",
                "        with torch.no_grad(): \n",
                "            out = model(**inp, output_attentions=True)\n",
                "            # FIX: Measure effect at LAYER_IDX + 1 (the layer AFFECTED by the steer)\n",
                "            adj = out.attentions[LAYER_IDX + 1][0].mean(dim=0).float().cpu().numpy()\n",
                "            f_base = get_fiedler_value(adj, gc, sa)\n",
                "            \n",
                "        # Active Ref\n",
                "        inp_a = tokenizer(active, return_tensors=\"pt\").to(model.device)\n",
                "        with torch.no_grad():\n",
                "            out_a = model(**inp_a, output_attentions=True)\n",
                "            adj_a = out_a.attentions[LAYER_IDX + 1][0].mean(dim=0).float().cpu().numpy()\n",
                "            f_act = get_fiedler_value(adj_a, gc, sa)\n",
                "\n",
                "        # Steered\n",
                "        h = model.model.layers[LAYER_IDX].register_forward_hook(steer_hook)\n",
                "        ppl_steer = calculate_perplexity(model, tokenizer, passive)\n",
                "        with torch.no_grad():\n",
                "            out = model(**inp, output_attentions=True)\n",
                "            adj = out.attentions[LAYER_IDX + 1][0].mean(dim=0).float().cpu().numpy()\n",
                "            f_steer = get_fiedler_value(adj, gc, sa)\n",
                "        h.remove()\n",
                "        \n",
                "        ppl_bases.append(ppl_base)\n",
                "        ppl_steers.append(ppl_steer)\n",
                "        f_bases.append(f_base)\n",
                "        f_steers.append(f_steer)\n",
                "        f_actives.append(f_act)\n",
                "\n",
                "    # METRICS\n",
                "    mean_ppl_base = np.mean(ppl_bases)\n",
                "    mean_ppl_steer = np.mean(ppl_steers)\n",
                "    mean_f_base = np.mean(f_bases)\n",
                "    mean_f_steer = np.mean(f_steers)\n",
                "    mean_f_act = np.mean(f_actives)\n",
                "    \n",
                "    pct_gain = (mean_ppl_base - mean_ppl_steer) / mean_ppl_base * 100\n",
                "    loss_gap = mean_f_act - mean_f_base\n",
                "    rec_pct = (mean_f_steer - mean_f_base) / loss_gap * 100 if loss_gap > 0 else 0\n",
                "    \n",
                "    print(f\"  PPL Gain: +{pct_gain:.2f}%\")\n",
                "    print(f\"  Fiedler: {mean_f_base:.3f} -> {mean_f_steer:.3f} (Active {mean_f_act:.3f})\")\n",
                "    print(f\"  Recovery: {rec_pct:.2f}%\")\n",
                "    \n",
                "    RESULTS.append({\n",
                "        \"Model\": model_name,\n",
                "        \"Base PPL\": mean_ppl_base,\n",
                "        \"Steered PPL\": mean_ppl_steer,\n",
                "        \"PPL Gain %\": pct_gain,\n",
                "        \"Recov %\": rec_pct\n",
                "    })"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 4. DISPLAY TABLE ---\n",
                "import pandas as pd\n",
                "df = pd.DataFrame(RESULTS)\n",
                "display(df)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}